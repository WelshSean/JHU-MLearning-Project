---
title: "Quantified Self Movement - Quality of Exercise"
author: "Sean Clarke"
date: "14 September 2015"
output: html_document
---
```{r}
library(caret)
library(randomForest)
library(rpart)
set.seed(1974)
```

# Data Acquisition

```{r}
# Download the data if we dont have it, then load te training data
DataDir <- "Data"
FilePathTrain <- paste(DataDir, "pml-training.csv", sep="/")
FilePathTest <- paste(DataDir, "pml-testing.csv", sep="/")
dir.create(DataDir, showWarnings=FALSE) 

## Only download if the file doesnt exist
if (! file.exists(FilePathTrain)){
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",FilePathTrain , method="curl")
}
if (! file.exists(FilePathTest)){
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",FilePathTest , method="curl")
}
pmltraining <- read.csv(FilePathTrain, stringsAsFactors = FALSE, na.strings = c("", "NA", "#DIV/0!"))
```

# Exploratory Analysis

We begin by exploring the data in order to get a feel for it.
```{r}
dim(pmltraining)
str(pmltraining)
head(pmltraining,5)
```


# Data preparation



When we read in the data, we tidy up some of the mess including "DIV/0!" and null cells by using the na.strings argument to read.csv. After doing this, we remove the first 7 columns as they contain extraneous tracking information and we also remove the  columns that contain nothing but NAs so we remove those and we convert classes to factors

```{r}
pmltraining <- pmltraining[-(1:7)]
pmltraining <- pmltraining[, colSums(is.na(pmltraining)) != nrow(pmltraining)]
pmltraining$classe <- factor(pmltraining$classe)
```

If we look at the variables in the dataset, we see that some summarised data is included (maximum/minimum/mean.standard deviation/variance) - we proceed to remove these variables from the dataset.

```{r}
columnsToRemove <- grep("^var.*|^stddev.*|^max.*|^min.*|^avg.*|^total.*|.*timestamp.*|user_name",names(pmltraining))
pmltraining <- pmltraining[-columnsToRemove]
```

Next we remove any predictors with near-zero variance.

```{r}
last <- ncol(pmltraining)
nzv <- nearZeroVar(pmltraining[,-last], saveMetrics=FALSE)
pmltraining <- pmltraining[,-nzv]
```

We see that there are still a considerable number of missing values

# Model Building

Fit Initial Model in order to identify the importance of the predictors. We will use k-fold validation in order to allow us to cross-validate the model to improve the accuracy.

We notice that we have a considerable number of missing values still.

```{r}
nrow(pmltraining)
colSums(is.na(pmltraining))
```

We remove colums with more than 90% missing values

```{r}
pmltraining <- pmltraining[, colSums(is.na(pmltraining))/nrow(pmltraining) <= .9]
colSums(is.na(pmltraining))
```

This leaves us with no missing values which is pleasing as all the elimination of predictors that we have done has been based on pretty stringent requirements - we havent easily thrown data away. This also means that we dont need to do any imputation.


COmpare RF and RPART

```{r}
#Creating a progress bar to know the status of CV
#progress.bar <- create_progress_bar("text")
#progress.bar$init(k)
 observed <- c()
 predicted <- c()
 folds <- createFolds(y=pmltraining$classe, k=10, list = TRUE, returnTrain = TRUE)
 for (i in 1:10){
   foldtrain <- pmltraining[folds[[i]],]
   foldtest <- pmltraining[-folds[[i]],]
   last <- ncol(foldtest)
   #foldfit <- train(classe ~ ., data = foldtrain, method="rf", preProcess = NULL)
   foldfit <- randomForest(classe ~ ., data=foldtrain, method="class")
   observed <- c(observed, foldtest$classe)
   predicted <- c(predicted, predict(foldfit, foldtest[-last]))
   print(i)
 }
results <- as.data.frame(cbind(observed,predicted))
names(results) <- c("observed", "predicted")
results$Difference <- abs(results$observed - results$predicted)
summary(results$Difference)
sqrt(mean((results$observed - results$predicted)^2))
```

```{r}
#Creating a progress bar to know the status of CV
#progress.bar <- create_progress_bar("text")
#progress.bar$init(k)
 observed <- c()
 predicted <- c()
 folds <- createFolds(y=pmltraining$classe, k=10, list = TRUE, returnTrain = TRUE)
 for (i in 1:10){
   foldtrain <- pmltraining[folds[[i]],]
   foldtest <- pmltraining[-folds[[i]],]
   last <- ncol(foldtest)
   #foldfit <- train(classe ~ ., data = foldtrain, method="rf", preProcess = NULL)
   foldfit <- rpart(classe ~ ., data=foldtrain, method="class")
   observed <- c(observed, foldtest$classe)
   predicted <- c(predicted, predict(foldfit, foldtest[-last]))
   print(i)
 }
results <- as.data.frame(cbind(observed,predicted))
names(results) <- c("observed", "predicted")
results$Difference <- abs(results$observed - results$predicted)
summary(results$Difference)
sqrt(mean((results$observed - results$predicted)^2))
```


#```{r}
#tfit <- train(classe ~ ., data=pmltraining, method="rpart")
#tp <- predict(tfit, pmltraining, na.action = na.pass)
#```

#```{r}
#testfit <- train(classe ~ ., data = pmltraining, method="rf",importance=TRUE)
##varImp(testfit)[[1]][1:20,]
#vi <- varImp(testfit, scale=FALSE)
#```

#```{r}
#str(pmltraining)
#names(pmltraining)
#```

