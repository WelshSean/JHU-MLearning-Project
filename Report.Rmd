---
title: "Quantified Self Movement - Quality of Exercise"
author: "Sean Clarke"
date: "14 September 2015"
output: html_document
---
```{r echo=FALSE,warning=FALSE,error=FALSE, message=FALSE}
library(caret)
library(randomForest)
library(rpart)
library(ggplot2)
library(cowplot)
set.seed(1974)
```
# Introduction

## Assignment details

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 

## Further details

[This publication](http://groupware.les.inf.puc-rio.br/work.jsf?p1=10335) is the main reference from the group that produced the data.

The dataset was generated by instrumenting test subjects with accelerometers and then having them perform bicep curls according to the specification referred to by the "classe" variable in the data. These map as follows:

- **A** exactly according to the specification
- **B** throwing the elbows to the front 
- **C** Lifting the dumbbell only halfway 
- **D** lowering the dumbbell only halfway 
- **E** throwing the hips to the front .


# Data Acquisition

The data using in this report, will be automatically downloaded from the following URLs

[Training Data](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)

[Test Data](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

A full discussion of the data cabe be found on the [Groupware@LES Website](http://groupware.les.inf.puc-rio.br/har)

```{r echo=FALSE,warning=FALSE,error=FALSE, message=FALSE}
# Download the data if we dont have it, then load te training data
DataDir <- "Data"
FilePathTrain <- paste(DataDir, "pml-training.csv", sep="/")
FilePathTest <- paste(DataDir, "pml-testing.csv", sep="/")
dir.create(DataDir, showWarnings=FALSE) 

## Only download if the file doesnt exist
if (! file.exists(FilePathTrain)){
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",FilePathTrain , method="curl")
}
if (! file.exists(FilePathTest)){
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",FilePathTest , method="curl")
}
pmltraining <- read.csv(FilePathTrain, stringsAsFactors = FALSE, na.strings = c("", "NA", "#DIV/0!"))
```

# Exploratory Analysis

We begin by exploring the data in order to get a feel for it - as the datasets are so large, the output from the Analysis is not included in this report, but the code is in the Github repository to allow full reproduceability.

```{r eval=FALSE}
dim(pmltraining)
str(pmltraining)
head(pmltraining,5)
```


# Data preparation

When we read in the data, we tidy up some of the mess including "DIV/0!" and null cells by using the na.strings argument to read.csv. After doing this, we remove the first 7 columns as they contain extraneous tracking information and we also remove the  columns that contain nothing but NAs so we remove those and we convert classes to factors

```{r}
pmltraining <- pmltraining[-(1:7)]
pmltraining <- pmltraining[, colSums(is.na(pmltraining)) != nrow(pmltraining)]
pmltraining$classe <- factor(pmltraining$classe)
```

If we look at the variables in the dataset, we see that some summarised data is included (maximum/minimum/mean.standard deviation/variance) - we proceed to remove these variables from the dataset.

```{r}
columnsToRemove <- grep("^var.*|^stddev.*|^max.*|^min.*|^avg.*|^total.*|.*timestamp.*|user_name",names(pmltraining))
pmltraining <- pmltraining[-columnsToRemove]
```

Next we remove any predictors with near-zero variance.

```{r}
last <- ncol(pmltraining)
nzv <- nearZeroVar(pmltraining[,-last], saveMetrics=FALSE)
pmltraining <- pmltraining[,-nzv]
```

We notice that we have a considerable number of missing values still.

```{r}
nrow(pmltraining)
colSums(is.na(pmltraining))
```

We remove colums with more than 90% missing values

```{r}
pmltraining <- pmltraining[, colSums(is.na(pmltraining))/nrow(pmltraining) <= .9]
colSums(is.na(pmltraining))
```

This leaves us with no missing values which is pleasing as all the elimination of predictors that we have done has been based on pretty stringent requirements - we havent easily thrown data away. This also means that we dont need to do any imputation.

# Model Building

We set out to determine whether to use Recursive Partitioning or the Random Forest algorithm to build our model. We also want to gain a feel for how effective the algorithm is and to attempt to avoid over-fitting. In order to achieve these aims, we will carry out cross-validation using k-fold validation.

In order to do this, we create folds in the data which we then loop through. During each iteration we build a training set from everything contained in the fold and  testing set from all the data that isnt included in the fold. We build a model using the training set and predict the test set and store the predictions and true values for classe on each iteration. We can use the resulting data set to estimate and visualise errors.

## Random Forest

```{r cache=TRUE}
 observed <- c()
 predicted <- c()
 folds <- createFolds(y=pmltraining$classe, k=10, list = TRUE, returnTrain = TRUE)
 for (i in 1:10){
   foldtrain <- pmltraining[folds[[i]],]
   foldtest <- pmltraining[-folds[[i]],]
   last <- ncol(foldtest)
   foldfit <- randomForest(classe ~ ., data=foldtrain, method="class")
   observed <- c(observed, foldtest$classe)
   predicted <- c(predicted, predict(foldfit, foldtest[-last]))
 }
resultsRF <- as.data.frame(cbind(observed,predicted))
names(resultsRF) <- c("observed", "predicted")
resultsRF$Difference <- abs(resultsRF$observed - resultsRF$predicted)
summary(resultsRF$Difference)
sqrt(mean((resultsRF$observed - resultsRF$predicted)^2))
nrow(subset(resultsRF, resultsRF$observed==resultsRF$predicted))*100/nrow(resultsRF)
```

We evaluate the Root Mean Square error between the predicted values and corresponding actual values in consolidated data that we return from the k-fold validation. We use the root mean square(RMS) error to deal with the fact that we get both positive and negative differences. We see that the RMS error is small which suggests that our Random Forest based model is doing a good job of predicting values. We also plot predicted versus observed values of classe from the data recorded during the cross-validation in Figure 1. We see that for the Random Forest algorithm a large proportion(99.7%) of the predicted values lie on the perfect prediction line shown in red in the figure.


```{r cache=TRUE}
 observed <- c()
 predicted <- c()
 folds <- createFolds(y=pmltraining$classe, k=10, list = TRUE, returnTrain = TRUE)
 for (i in 1:10){
   foldtrain <- pmltraining[folds[[i]],]
   foldtest <- pmltraining[-folds[[i]],]
   last <- ncol(foldtest)
   foldfit <- rpart(classe ~ ., data=foldtrain, method="class")
   observed <- c(observed, foldtest$classe)
   predicted <- c(predicted, predict(foldfit, foldtest[-last], type="class"))
 }
resultsRP <- as.data.frame(cbind(observed,predicted))
names(resultsRP) <- c("observed", "predicted")
resultsRP$Difference <- abs(resultsRP$observed - resultsRP$predicted)
summary(resultsRP$Difference)
sqrt(mean((resultsRP$observed - resultsRP$predicted)^2))
nrow(subset(resultsRP, resultsRP$observed==resultsRP$predicted))*100/nrow(resultsRP)
```

We carried out the same evaluations for a model built using the Recursive partitioning algorithm and it was clear  that this algorithm as not as successful in modelling the data. The predicted versus observed plot clearly shows a much wider variation in values predicted - they were not clustered aroudn the ideal "y=x" line as much and indeed only 74.2% of the predictions were equal to the actual value when using this algorithm. The calculated RMS errors were also an order of magnitude larger.

The figure below shows the difference in the quality in fit between the two models - it should be noted that Jitter was added to the plots in order to illustrate the number of points - the points actually all fall on integer intersection points.

```{r echo=FALSE,warning=FALSE,error=FALSE,message=FALSE,fig.width=8}
plot1 <- ggplot(resultsRF, aes(observed, predicted)) + geom_point()
plot1 <- plot1 + geom_jitter(position = position_jitter(width = .08, height=0.08)) 
plot1 <- plot1 + geom_abline(intercept = 0, slope = 1, color="red")
plot1 <- plot1 +  theme(plot.title = element_text(size=12))
plot1 <- plot1 + ggtitle("Values of the classe variable predicted using the Random Forest Algorithm versus \n the actual values from the relevant observations in the cross-validation testing data sets")

plot2 <- ggplot(resultsRP, aes(observed, predicted)) + geom_point()
plot2 <- plot2 + geom_jitter(position = position_jitter(width = .08, height=0.08)) 
plot2 <- plot2 + geom_abline(intercept = 0, slope = 1, color="red")
plot2 <- plot2 +  theme(plot.title = element_text(size=12))
plot2 <- plot2 + ggtitle("Values of the classe variable predicted using the Recursive Partitioning Algorithm versus \n the actual values from the relevant observations in the cross-validation testing data sets")

plot_grid(plot1, plot2, ncol = 1, align = 'v')
```

Based on the above analysis, we proceed using the Random Forest algorithm to build out final model.

# Model Building

We now proceed to build our model using the full training data set.

```{r cache=TRUE}
model <- randomForest(classe ~ ., data=pmltraining, method="class")
```

We proceed to look at the effectiveness of predicting the values that we already know from the training set which will allow us to estimate the **In Sample Error**.

```{r}
last <- ncol(pmltraining)
insamppred <- predict(model, pmltraining[-last])
insamppred <- as.data.frame(cbind(insamppred, pmltraining$classe))
names(insamppred) <- c("Predicted", "Observed")
CFM <- confusionMatrix(insamppred$Predicted, insamppred$Observed)
CFM$table
```

So as we can see from the Confusion Matrix, the model is extremely good at fitting the data in the training set, so much so that the In Sample Error is 0. This does lead to some concerns around potential over-fitting. Obviously the In Sample Error isnt much help in determing the **Out of Sample Error** as all we can say is that it is greater than zero!!!! 

However, all is not lost - part of the reason that we did the k-fold cross-validation was to get an estimate for the out of sample error rate. So based on that estimate, we can say that the Out of Sample Root Mean Square Error is going to be at least 0.07. Using the estimates carried out we can also say that at least 0.4% of our predictions will be erroneous.


# Prediction

We now read in our test set and clean up as necessary in a consistent manner to what we did for the training data set.

```{r}
pmltest <- read.csv(FilePathTest, stringsAsFactors = FALSE, na.strings = c("", "NA", "#DIV/0!"))
pmltest <- pmltest[-(1:7)]
FinalPredictions <- predict(model, pmltest)
summary(FinalPredictions)
```

# Conclusions

Out of the two models that we considered, the Random Forest algorithm is clearly the most effective at modelling the training set. Care will need to be taken when applying the model to the test dataset in cae this was down to overfitting.